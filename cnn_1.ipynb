{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from scipy.signal import resample\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution1D, Dropout, BatchNormalization, MaxPooling1D\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam, SGD, Adagrad\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data loading\n",
    "datapath = Path('../dataset')\n",
    "xfile = 'X_features_spec.npy'\n",
    "yfile = 'Y_labels_spec.npy'\n",
    "def load_waveforms():\n",
    "    X_list = np.load(str(datapath.joinpath(xfile)))\n",
    "    Y_list = np.load(str(datapath.joinpath(yfile)))\n",
    "    return X_list, Y_list\n",
    "\n",
    "def positve_samples(xlist):\n",
    "    ## some samples have negative signs\n",
    "    xl_new = []\n",
    "    for sample in range(xlist.shape[0]):\n",
    "        points = xlist[sample]\n",
    "        for p in range(points.shape[0]):\n",
    "            point = points[p]\n",
    "            if np.sum(point) < 0:\n",
    "                points[p] = -point\n",
    "        xl_new.append(points)\n",
    "    return np.array(xl_new)\n",
    "\n",
    "## apply to loaded dataset\n",
    "def split_by_channel(xlist):\n",
    "    ## input as (n, 2500)\n",
    "    def standard_resample(arr):\n",
    "        return resample(arr, 2500)\n",
    "    ## if some is not with dim 625, resample it\n",
    "    xl_new = []\n",
    "    for sample in range(xlist.shape[0]):\n",
    "        points = xlist[sample]\n",
    "        if points.shape[1] != 2500:\n",
    "            print(\"resample\")\n",
    "            print(points.shape)\n",
    "            points = np.apply_along_axis(standard_resample, axis=1, arr=points)\n",
    "        points = points.reshape((points.shape[0], 625, 4))\n",
    "        xl_new.append(points)\n",
    "    return np.array(xl_new)\n",
    "\n",
    "## input is after split\n",
    "def apply_resample(xlist, outdim):\n",
    "    ## resample\n",
    "    def resample_waveform(arr):\n",
    "        ## arr.shape: (indim, )\n",
    "        return resample(arr, outdim)\n",
    "    xl_new = []\n",
    "    for sample in range(xlist.shape[0]):\n",
    "        points = xlist[sample]\n",
    "        points = np.apply_along_axis(resample_waveform, axis=1, arr=points)\n",
    "        xl_new.append(points)\n",
    "    return np.array(xl_new)\n",
    "\n",
    "## input is combined exp. (18000 ,625, 4)\n",
    "def get_xtrain_mean(x_train):\n",
    "    ## mean value for each dimension (exp. each of 625 dim)\n",
    "    m = np.mean(x_train, axis=0)\n",
    "    ## then we can apply x_train - m for zero mean\n",
    "    return m\n",
    "\n",
    "## input is after split\n",
    "## one variance for each channel\n",
    "def normalize_waveform():\n",
    "    ## we don't necessarily need this\n",
    "    pass\n",
    "\n",
    "def combine_samples(arrs):\n",
    "    ## exp. arrs.shape: (20, ?)\n",
    "    pass\n",
    "\n",
    "def binary_label(ylist):\n",
    "    ## 1, 2 --> 1\n",
    "    ylist_new = []\n",
    "    for sample in range(ylist.shape[0]):\n",
    "        labels = ylist[sample]\n",
    "        labels[labels > 1] = 1\n",
    "        ylist_new.append(labels)\n",
    "    return np.array(ylist_new)\n",
    "\n",
    "def combine_samples(arrs):\n",
    "    ## exp. arrs.shape: (20, ?)\n",
    "    if arrs.shape[0] < 1:\n",
    "        return arrs\n",
    "    sp = list(arrs[0].shape)\n",
    "    sp[0] = 0\n",
    "    combined = np.zeros(sp)\n",
    "    print(\"combinde\", combined.shape)\n",
    "    for sample in range(arrs.shape[0]):\n",
    "        arr = arrs[sample]\n",
    "        combined = np.concatenate((combined, arr), axis=0)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## global parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### models to test\n",
    "* VERY DEEP CONVOLUTIONAL NEURAL NETWORKS FOR RAW WAVEFORMS\n",
    "\n",
    "they have a clearly defined structure, and their data are of similar dimentions\n",
    "\n",
    "* Raw Waveform-based Audio Classification Using Sample-level CNN Architectures\n",
    "* SAMPLE-LEVEL DEEP CONVOLUTIONAL NEURAL NETWORKS FOR MUSIC AUTO-TAGGING USING RAW WAVEFORMS\n",
    "\n",
    "realtively simple arch;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test raw waveform input first\n",
    "* input: 2500 * 1 waveform (normalized, center to 0, variance 1)\n",
    "* conv layer: with/withour overlapping. In the paper:\n",
    "    * filter size 3, stride 3, 128 filters\n",
    "    * filter size 80, stride 4, 256 filters\n",
    "* batch normalization: after every conv layer\n",
    "* max pool\n",
    "    * stride of 3? 4?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input\n",
    "or we can make the input as 625 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model\n",
    "## 1d conv, size 4 filter, 64 filters, stride 2\n",
    "## output 1250 * 64\n",
    "## batch norm\n",
    "## maxpool 2 * 1\n",
    "## output 625 * 64\n",
    "## 1d conv, size 3 filter, stride 2, 128 filters\n",
    "## maxpool 2 * 1\n",
    "## output 312 * 64\n",
    "## 1d conv, size 3 filter, stride 3, 128 filters\n",
    "## output 104 * 128\n",
    "## maxpool 2 * 1\n",
    "## output 52 * 128\n",
    "## 1d conv, size 3 filter, stride 2, 256 filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## one time parameter for the model below\n",
    "## regularizer\n",
    "## l2\n",
    "ker_reg = 0.1\n",
    "act_reg = 0.1\n",
    "## kernel_initializer\n",
    "ker_init = initializers.glorot_normal(seed=None)\n",
    "## shape\n",
    "in_shape = (648, 4)\n",
    "## learning rate\n",
    "opt = Adam()\n",
    "opt.lr = 0.0001\n",
    "##\n",
    "OUTPUT_SIZE = 2\n",
    "## batch size\n",
    "bsize = 50\n",
    "##\n",
    "epochs = 30\n",
    "## callback\n",
    "model_callback = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model\n",
    "## resample data to 648 * 1\n",
    "model = Sequential()\n",
    "## 1d conv, size 3 filter, 64 filters, stride 1\n",
    "## batch norm, batch after activation\n",
    "## no maxpool\n",
    "## keras.layers.Conv1D(filters, kernel_size, strides=1, padding='valid', dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "model.add(Convolution1D(filters=64, kernel_size=3, strides=1, padding='same', input_shape=in_shape, kernel_initializer=ker_init, activation='relu', kernel_regularizer=regularizers.l2(ker_reg)))\n",
    "model.add(BatchNormalization())\n",
    "## 1d conv, size 3 filter, 128 filters, stride 1\n",
    "## batch norm, batch after activation\n",
    "## maxpool 3 --> 216 * 128\n",
    "model.add(Convolution1D(filters=128, kernel_size=3, strides=1, padding='same', input_shape=in_shape, activation='relu', kernel_regularizer=regularizers.l2(ker_reg)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "## 1d conv, size 3 filter, 128 filters, stride 2\n",
    "## batch norm, batch after activation\n",
    "## max pool 3 -->  36 * 128\n",
    "model.add(Convolution1D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu', kernel_regularizer=regularizers.l2(ker_reg)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "## 1d conv, size 3 filter, 256 filters, stride 2\n",
    "## batch norm, batch after activation\n",
    "## max pool 3 -->  6 * 256\n",
    "model.add(Convolution1D(filters=256, kernel_size=3, strides=2, padding='same', activation='relu', kernel_regularizer=regularizers.l2(ker_reg)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "## 1d conv, size 3 filter, 512 filters, stride 2\n",
    "## batch norm, batch after activation\n",
    "## max pool 3 -->  1 * 512\n",
    "model.add(Convolution1D(filters=512, kernel_size=3, strides=2, padding='same',activation='relu', kernel_regularizer=regularizers.l2(ker_reg)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "##\n",
    "model.add(Flatten())\n",
    "## fully connected\n",
    "model.add(Dense(OUTPUT_SIZE))\n",
    "## softmax\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_11 (Conv1D)           (None, 648, 64)           832       \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 648, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 648, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 648, 128)          512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 216, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 108, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 108, 128)          512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 36, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 18, 256)           98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 18, 256)           1024      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 6, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 3, 512)            393728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 3, 512)            2048      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 1, 512)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 1026      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 572,482\n",
      "Trainable params: 570,306\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resample\n",
      "(2490, 1920)\n",
      "resample\n",
      "(878, 1920)\n",
      "(2394, 648, 4)\n",
      "(4144, 648, 4)\n",
      "(3302, 648, 4)\n",
      "(1272, 648, 4)\n",
      "(389, 648, 4)\n",
      "(2716, 648, 4)\n",
      "(61, 648, 4)\n",
      "(628, 648, 4)\n",
      "(611, 648, 4)\n",
      "(771, 648, 4)\n",
      "(201, 648, 4)\n",
      "(1354, 648, 4)\n",
      "(2490, 648, 4)\n",
      "(878, 648, 4)\n",
      "(2506, 648, 4)\n",
      "(1688, 648, 4)\n",
      "(2067, 648, 4)\n",
      "(1554, 648, 4)\n",
      "(635, 648, 4)\n",
      "(1439, 648, 4)\n"
     ]
    }
   ],
   "source": [
    "## experiment\n",
    "## case 12, 14 for test\n",
    "## case 18 for validation\n",
    "## other case for traning\n",
    "x_list, y_list = load_waveforms()\n",
    "x_list = positve_samples(x_list)\n",
    "x_list = split_by_channel(x_list)\n",
    "x_list = apply_resample(x_list, 648)\n",
    "y_list = binary_label(y_list)\n",
    "for i in range(x_list.shape[0]):\n",
    "    print(x_list[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combinde (0, 648, 4)\n",
      "combinde (0,)\n",
      "combinde (0, 648, 4)\n",
      "combinde (0,)\n",
      "combinde (0, 648, 4)\n",
      "combinde (0,)\n"
     ]
    }
   ],
   "source": [
    "val_idx = [17]\n",
    "test_idx = [11, 13]\n",
    "train_list_x = []\n",
    "train_list_y = []\n",
    "val_list_x = []\n",
    "val_list_y = []\n",
    "test_list_x = []\n",
    "test_list_y = []\n",
    "for idx in range(x_list.shape[0]):\n",
    "    if idx not in (val_idx + test_idx):\n",
    "        train_list_x.append(x_list[idx])\n",
    "        train_list_y.append(y_list[idx])\n",
    "        \n",
    "for idx in val_idx:\n",
    "    val_list_x.append(x_list[idx])\n",
    "    val_list_y.append(y_list[idx])\n",
    "    \n",
    "for idx in test_idx:\n",
    "    test_list_x.append(x_list[idx])\n",
    "    test_list_y.append(y_list[idx])  \n",
    "\n",
    "train_list_x = np.array(train_list_x)\n",
    "train_list_y = np.array(train_list_y)\n",
    "val_list_x = np.array(val_list_x)\n",
    "val_list_y = np.array(val_list_y)\n",
    "test_list_x = np.array(test_list_x)\n",
    "test_list_y = np.array(test_list_y)\n",
    "train_list_x = combine_samples(train_list_x)\n",
    "train_list_y = combine_samples(train_list_y)\n",
    "val_list_x = combine_samples(val_list_x)\n",
    "val_list_y = combine_samples(val_list_y)\n",
    "test_list_x = combine_samples(test_list_x)\n",
    "test_list_y = combine_samples(test_list_y)\n",
    "train_list_y = np_utils.to_categorical(train_list_y, num_classes=2)\n",
    "val_list_y = np_utils.to_categorical(val_list_y, num_classes=2)\n",
    "test_list_y = np_utils.to_categorical(test_list_y, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27314 samples, validate on 2232 samples\n",
      "Epoch 1/30\n",
      " - 15s - loss: 28.8543 - acc: 0.9401 - val_loss: 9.9680 - val_acc: 0.3342\n",
      "Epoch 2/30\n",
      " - 15s - loss: 4.4028 - acc: 0.9463 - val_loss: 3.4727 - val_acc: 0.4350\n",
      "Epoch 3/30\n",
      " - 15s - loss: 1.2810 - acc: 0.9487 - val_loss: 1.8393 - val_acc: 0.3925\n",
      "Epoch 4/30\n",
      " - 15s - loss: 0.5820 - acc: 0.9499 - val_loss: 1.3819 - val_acc: 0.5506\n",
      "Epoch 5/30\n",
      " - 15s - loss: 0.3583 - acc: 0.9516 - val_loss: 1.0141 - val_acc: 0.6577\n",
      "Epoch 6/30\n",
      " - 15s - loss: 0.2699 - acc: 0.9534 - val_loss: 1.1281 - val_acc: 0.5981\n",
      "Epoch 7/30\n",
      " - 15s - loss: 0.2281 - acc: 0.9535 - val_loss: 1.3551 - val_acc: 0.3678\n",
      "Epoch 8/30\n",
      " - 15s - loss: 0.2077 - acc: 0.9545 - val_loss: 2.0228 - val_acc: 0.2509\n",
      "Epoch 9/30\n",
      " - 15s - loss: 0.1943 - acc: 0.9551 - val_loss: 1.6658 - val_acc: 0.3513\n",
      "Epoch 10/30\n",
      " - 15s - loss: 0.1771 - acc: 0.9582 - val_loss: 1.7969 - val_acc: 0.3297\n",
      "Epoch 11/30\n",
      " - 15s - loss: 0.1704 - acc: 0.9576 - val_loss: 1.7024 - val_acc: 0.3866\n",
      "Epoch 12/30\n",
      " - 15s - loss: 0.1631 - acc: 0.9599 - val_loss: 2.2193 - val_acc: 0.2805\n",
      "Epoch 13/30\n",
      " - 15s - loss: 0.1587 - acc: 0.9606 - val_loss: 1.9229 - val_acc: 0.3297\n",
      "Epoch 14/30\n",
      " - 15s - loss: 0.1575 - acc: 0.9604 - val_loss: 2.2874 - val_acc: 0.2845\n",
      "Epoch 15/30\n",
      " - 15s - loss: 0.1569 - acc: 0.9614 - val_loss: 1.5435 - val_acc: 0.3450\n",
      "Epoch 16/30\n",
      " - 15s - loss: 0.1521 - acc: 0.9614 - val_loss: 1.6834 - val_acc: 0.4149\n",
      "Epoch 17/30\n",
      " - 15s - loss: 0.1480 - acc: 0.9632 - val_loss: 2.3002 - val_acc: 0.4014\n",
      "Epoch 18/30\n",
      " - 15s - loss: 0.1447 - acc: 0.9627 - val_loss: 1.8149 - val_acc: 0.4736\n",
      "Epoch 19/30\n",
      " - 15s - loss: 0.1437 - acc: 0.9636 - val_loss: 1.7923 - val_acc: 0.2966\n",
      "Epoch 20/30\n",
      " - 15s - loss: 0.1408 - acc: 0.9635 - val_loss: 1.6466 - val_acc: 0.3826\n",
      "Epoch 21/30\n",
      " - 15s - loss: 0.1385 - acc: 0.9646 - val_loss: 2.0463 - val_acc: 0.3199\n",
      "Epoch 22/30\n",
      " - 15s - loss: 0.1341 - acc: 0.9653 - val_loss: 2.0007 - val_acc: 0.2760\n",
      "Epoch 23/30\n",
      " - 15s - loss: 0.1370 - acc: 0.9647 - val_loss: 2.2248 - val_acc: 0.4135\n",
      "Epoch 24/30\n",
      " - 15s - loss: 0.1347 - acc: 0.9654 - val_loss: 2.0273 - val_acc: 0.2881\n",
      "Epoch 25/30\n",
      " - 15s - loss: 0.1340 - acc: 0.9636 - val_loss: 1.5754 - val_acc: 0.3687\n",
      "Epoch 26/30\n",
      " - 15s - loss: 0.1338 - acc: 0.9650 - val_loss: 1.7610 - val_acc: 0.4319\n",
      "Epoch 27/30\n",
      " - 15s - loss: 0.1335 - acc: 0.9651 - val_loss: 1.5929 - val_acc: 0.4194\n",
      "Epoch 28/30\n",
      " - 15s - loss: 0.1305 - acc: 0.9658 - val_loss: 1.6856 - val_acc: 0.2917\n",
      "Epoch 29/30\n",
      " - 15s - loss: 0.1277 - acc: 0.9654 - val_loss: 2.1662 - val_acc: 0.3992\n",
      "Epoch 30/30\n",
      " - 15s - loss: 0.1298 - acc: 0.9656 - val_loss: 1.6222 - val_acc: 0.3333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fff3289bc18>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_list_x, train_list_y,\n",
    "          epochs=epochs,\n",
    "          batch_size=bsize\n",
    "          verbose=2,\n",
    "          validation_data=(test_list_x, test_list_y),\n",
    "          callbacks=[model_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2232/2232 [==============================] - 0s 191us/step\n",
      "loss 1.622199524047127 acc 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_list_x, test_list_y)\n",
    "print(\"loss\", loss, \"acc\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simpletensor",
   "language": "python",
   "name": "simpletensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
