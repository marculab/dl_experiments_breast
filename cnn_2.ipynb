{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from scipy.signal import resample\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution1D, Dropout, BatchNormalization, MaxPooling1D, AveragePooling1D\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam, SGD, Adagrad\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cnn_1 seems too complex. Make a simpler model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data loading\n",
    "## data loading\n",
    "datapath = Path('../dataset')\n",
    "xfile = 'X_features_spec.npy'\n",
    "yfile = 'Y_labels_spec.npy'\n",
    "def load_waveforms():\n",
    "    X_list = np.load(str(datapath.joinpath(xfile)))\n",
    "    Y_list = np.load(str(datapath.joinpath(yfile)))\n",
    "    return X_list, Y_list\n",
    "\n",
    "def positve_samples(xlist):\n",
    "    ## some samples have negative signs\n",
    "    xl_new = []\n",
    "    for sample in range(xlist.shape[0]):\n",
    "        points = xlist[sample]\n",
    "        for p in range(points.shape[0]):\n",
    "            point = points[p]\n",
    "            if np.sum(point) < 0:\n",
    "                points[p] = -point\n",
    "        xl_new.append(points)\n",
    "    return np.array(xl_new)\n",
    "\n",
    "## apply to loaded dataset\n",
    "def split_by_channel(xlist):\n",
    "    ## input as (n, 2500)\n",
    "    def standard_resample(arr):\n",
    "        return resample(arr, 2500)\n",
    "    ## if some is not with dim 625, resample it\n",
    "    xl_new = []\n",
    "    for sample in range(xlist.shape[0]):\n",
    "        points = xlist[sample]\n",
    "        if points.shape[1] != 2500:\n",
    "            print(\"resample\")\n",
    "            print(points.shape)\n",
    "            points = np.apply_along_axis(standard_resample, axis=1, arr=points)\n",
    "        points = points.reshape((points.shape[0], 625, 4))\n",
    "        xl_new.append(points)\n",
    "    return np.array(xl_new)\n",
    "\n",
    "## input is after split\n",
    "def apply_resample(xlist, outdim):\n",
    "    ## resample\n",
    "    def resample_waveform(arr):\n",
    "        ## arr.shape: (indim, )\n",
    "        return resample(arr, outdim)\n",
    "    xl_new = []\n",
    "    for sample in range(xlist.shape[0]):\n",
    "        points = xlist[sample]\n",
    "        points = np.apply_along_axis(resample_waveform, axis=1, arr=points)\n",
    "        xl_new.append(points)\n",
    "    return np.array(xl_new)\n",
    "\n",
    "## input is combined exp. (18000 ,625, 4)\n",
    "def get_xtrain_mean(x_train):\n",
    "    ## mean value for each dimension (exp. each of 625 dim)\n",
    "    m = np.mean(x_train, axis=0)\n",
    "    ## then we can apply x_train - m for zero mean\n",
    "    return m\n",
    "\n",
    "## input is after split\n",
    "## one variance for each channel\n",
    "def normalize_waveform():\n",
    "    ## we don't necessarily need this\n",
    "    pass\n",
    "\n",
    "def combine_samples(arrs):\n",
    "    ## exp. arrs.shape: (20, ?)\n",
    "    pass\n",
    "\n",
    "def binary_label(ylist):\n",
    "    ## 1, 2 --> 1\n",
    "    ylist_new = []\n",
    "    for sample in range(ylist.shape[0]):\n",
    "        labels = ylist[sample]\n",
    "        labels[labels > 1] = 1\n",
    "        ylist_new.append(labels)\n",
    "    return np.array(ylist_new)\n",
    "\n",
    "def combine_samples(arrs):\n",
    "    ## exp. arrs.shape: (20, ?)\n",
    "    if arrs.shape[0] < 1:\n",
    "        return arrs\n",
    "    sp = list(arrs[0].shape)\n",
    "    sp[0] = 0\n",
    "    combined = np.zeros(sp)\n",
    "    print(\"combinde\", combined.shape)\n",
    "    for sample in range(arrs.shape[0]):\n",
    "        arr = arrs[sample]\n",
    "        combined = np.concatenate((combined, arr), axis=0)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## one time parameter for the model below\n",
    "## regularizer\n",
    "## l2\n",
    "ker_reg = 0.1\n",
    "act_reg = 0.1\n",
    "## kernel_initializer\n",
    "ker_init = initializers.glorot_normal(seed=None)\n",
    "## shape\n",
    "in_shape = (648, 4)\n",
    "## learning rate\n",
    "opt = Adam()\n",
    "opt.lr = 0.0001\n",
    "##\n",
    "OUTPUT_SIZE = 2\n",
    "##\n",
    "epochs = 20\n",
    "## callback\n",
    "model_callback = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this time, add a conv layer with bigger kernel size and stride at first --> more generalized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model\n",
    "## resample data to 640 * 4\n",
    "model = Sequential()\n",
    "## 1d conv, size 20 filter, 16 filters, stride 2\n",
    "## batch norm, batch after activation\n",
    "## maxpool 2 --> 160 * 16\n",
    "## keras.layers.Conv1D(filters, kernel_size, strides=1, padding='valid', dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "model.add(Convolution1D(filters=16, kernel_size=20, strides=2, padding='same', input_shape=in_shape, kernel_initializer=ker_init, activation='relu', kernel_regularizer=regularizers.l2(ker_reg)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "## 1d conv, size 4 filter, 32 filters, stride 2\n",
    "## batch norm, batch after activation\n",
    "## maxpool 3 --> 40 * 32\n",
    "model.add(Convolution1D(filters=32, kernel_size=4, strides=2, padding='same', input_shape=in_shape, activation='relu', kernel_regularizer=regularizers.l2(ker_reg)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "## 1d conv, size 3 filter, 128 filters, stride 2\n",
    "## batch norm, batch after activation\n",
    "## max pool 3 -->  10 * 64\n",
    "model.add(Convolution1D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', kernel_regularizer=regularizers.l2(ker_reg)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "## average pool 10 -->  1 * 64\n",
    "model.add(AveragePooling1D(pool_size=10))\n",
    "##\n",
    "model.add(Flatten())\n",
    "## fully connected\n",
    "model.add(Dense(OUTPUT_SIZE))\n",
    "## softmax\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 324, 16)           1296      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 324, 16)           64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 162, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 81, 32)            2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 81, 32)            128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 40, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 20, 64)            6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 20, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 10,162\n",
      "Trainable params: 9,938\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resample\n",
      "(2490, 1920)\n",
      "resample\n",
      "(878, 1920)\n",
      "(2394, 648, 4)\n",
      "(4144, 648, 4)\n",
      "(3302, 648, 4)\n",
      "(1272, 648, 4)\n",
      "(389, 648, 4)\n",
      "(2716, 648, 4)\n",
      "(61, 648, 4)\n",
      "(628, 648, 4)\n",
      "(611, 648, 4)\n",
      "(771, 648, 4)\n",
      "(201, 648, 4)\n",
      "(1354, 648, 4)\n",
      "(2490, 648, 4)\n",
      "(878, 648, 4)\n",
      "(2506, 648, 4)\n",
      "(1688, 648, 4)\n",
      "(2067, 648, 4)\n",
      "(1554, 648, 4)\n",
      "(635, 648, 4)\n",
      "(1439, 648, 4)\n"
     ]
    }
   ],
   "source": [
    "## experiment\n",
    "## case 12, 14 for test\n",
    "## case 18 for validation\n",
    "## other case for traning\n",
    "x_list, y_list = load_waveforms()\n",
    "x_list = positve_samples(x_list)\n",
    "x_list = split_by_channel(x_list)\n",
    "x_list = apply_resample(x_list, 648)\n",
    "y_list = binary_label(y_list)\n",
    "for i in range(x_list.shape[0]):\n",
    "    print(x_list[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combinde (0, 648, 4)\n",
      "combinde (0,)\n",
      "combinde (0, 648, 4)\n",
      "combinde (0,)\n",
      "combinde (0, 648, 4)\n",
      "combinde (0,)\n"
     ]
    }
   ],
   "source": [
    "val_idx = [1, 5]\n",
    "test_idx = [6]\n",
    "train_list_x = []\n",
    "train_list_y = []\n",
    "val_list_x = []\n",
    "val_list_y = []\n",
    "test_list_x = []\n",
    "test_list_y = []\n",
    "for idx in range(x_list.shape[0]):\n",
    "    if idx not in (val_idx + test_idx):\n",
    "        train_list_x.append(x_list[idx])\n",
    "        train_list_y.append(y_list[idx])\n",
    "        \n",
    "for idx in val_idx:\n",
    "    val_list_x.append(x_list[idx])\n",
    "    val_list_y.append(y_list[idx])\n",
    "    \n",
    "for idx in test_idx:\n",
    "    test_list_x.append(x_list[idx])\n",
    "    test_list_y.append(y_list[idx])  \n",
    "\n",
    "train_list_x = np.array(train_list_x)\n",
    "train_list_y = np.array(train_list_y)\n",
    "val_list_x = np.array(val_list_x)\n",
    "val_list_y = np.array(val_list_y)\n",
    "test_list_x = np.array(test_list_x)\n",
    "test_list_y = np.array(test_list_y)\n",
    "train_list_x = combine_samples(train_list_x)\n",
    "train_list_y = combine_samples(train_list_y)\n",
    "val_list_x = combine_samples(val_list_x)\n",
    "val_list_y = combine_samples(val_list_y)\n",
    "test_list_x = combine_samples(test_list_x)\n",
    "test_list_y = combine_samples(test_list_y)\n",
    "train_list_y = np_utils.to_categorical(train_list_y, num_classes=2)\n",
    "val_list_y = np_utils.to_categorical(val_list_y, num_classes=2)\n",
    "test_list_y = np_utils.to_categorical(test_list_y, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24179 samples, validate on 6860 samples\n",
      "Epoch 1/20\n",
      " - 8s - loss: 4.3996 - acc: 0.8738 - val_loss: 2.5144 - val_acc: 0.7608\n",
      "Epoch 2/20\n",
      " - 7s - loss: 1.5094 - acc: 0.9207 - val_loss: 1.2666 - val_acc: 0.7016\n",
      "Epoch 3/20\n",
      " - 7s - loss: 0.7768 - acc: 0.9279 - val_loss: 0.7952 - val_acc: 0.8751\n",
      "Epoch 4/20\n",
      " - 7s - loss: 0.5520 - acc: 0.9319 - val_loss: 1.1250 - val_acc: 0.4778\n",
      "Epoch 5/20\n",
      " - 7s - loss: 0.4522 - acc: 0.9335 - val_loss: 0.5303 - val_acc: 0.9204\n",
      "Epoch 6/20\n",
      " - 7s - loss: 0.3936 - acc: 0.9355 - val_loss: 0.7719 - val_acc: 0.7045\n",
      "Epoch 7/20\n",
      " - 7s - loss: 0.3550 - acc: 0.9366 - val_loss: 1.1549 - val_acc: 0.6327\n",
      "Epoch 8/20\n",
      " - 7s - loss: 0.3273 - acc: 0.9365 - val_loss: 0.5479 - val_acc: 0.8436\n",
      "Epoch 9/20\n",
      " - 7s - loss: 0.3080 - acc: 0.9381 - val_loss: 0.6187 - val_acc: 0.7475\n",
      "Epoch 10/20\n",
      " - 7s - loss: 0.2918 - acc: 0.9382 - val_loss: 0.4218 - val_acc: 0.8516\n",
      "Epoch 11/20\n",
      " - 7s - loss: 0.2803 - acc: 0.9387 - val_loss: 0.6574 - val_acc: 0.7194\n",
      "Epoch 12/20\n",
      " - 7s - loss: 0.2688 - acc: 0.9387 - val_loss: 0.4983 - val_acc: 0.8310\n",
      "Epoch 13/20\n",
      " - 7s - loss: 0.2586 - acc: 0.9402 - val_loss: 0.7685 - val_acc: 0.6672\n",
      "Epoch 14/20\n",
      " - 7s - loss: 0.2523 - acc: 0.9408 - val_loss: 1.6845 - val_acc: 0.6270\n",
      "Epoch 15/20\n",
      " - 7s - loss: 0.2477 - acc: 0.9394 - val_loss: 0.5643 - val_acc: 0.7880\n",
      "Epoch 16/20\n",
      " - 7s - loss: 0.2416 - acc: 0.9404 - val_loss: 0.7400 - val_acc: 0.7130\n",
      "Epoch 17/20\n",
      " - 7s - loss: 0.2353 - acc: 0.9416 - val_loss: 0.5046 - val_acc: 0.7911\n",
      "Epoch 18/20\n",
      " - 7s - loss: 0.2334 - acc: 0.9396 - val_loss: 0.4443 - val_acc: 0.8471\n",
      "Epoch 19/20\n",
      " - 7s - loss: 0.2273 - acc: 0.9419 - val_loss: 1.1118 - val_acc: 0.6321\n",
      "Epoch 20/20\n",
      " - 7s - loss: 0.2246 - acc: 0.9407 - val_loss: 0.4674 - val_acc: 0.8251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fff62d73d68>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_list_x, train_list_y,\n",
    "          epochs=epochs,\n",
    "          verbose=2,\n",
    "          validation_data=(val_list_x, val_list_y),\n",
    "          callbacks=[model_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 0s 203us/step\n",
      "loss 4.813035878978792 acc 0.01639344262295082\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_list_x, test_list_y)\n",
    "print(\"loss\", loss, \"acc\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simpletensor",
   "language": "python",
   "name": "simpletensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
