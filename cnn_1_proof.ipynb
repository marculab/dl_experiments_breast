{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from scipy.signal import resample\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution1D, Dropout, BatchNormalization, MaxPooling1D\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam, SGD, Adagrad\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import TensorBoard, CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data loading\n",
    "datapath = Path('../dataset')\n",
    "xfile = 'X_features_spec.npy'\n",
    "yfile = 'Y_labels_spec.npy'\n",
    "def load_waveforms():\n",
    "    X_list = np.load(str(datapath.joinpath(xfile)))\n",
    "    Y_list = np.load(str(datapath.joinpath(yfile)))\n",
    "    return X_list, Y_list\n",
    "\n",
    "def positve_samples(xlist):\n",
    "    ## some samples have negative signs\n",
    "    xl_new = []\n",
    "    for sample in range(xlist.shape[0]):\n",
    "        points = xlist[sample]\n",
    "        for p in range(points.shape[0]):\n",
    "            point = points[p]\n",
    "            if np.sum(point) < 0:\n",
    "                points[p] = -point\n",
    "        xl_new.append(points)\n",
    "    return np.array(xl_new)\n",
    "\n",
    "## apply to loaded dataset\n",
    "def split_by_channel(xlist):\n",
    "    ## input as (n, 2500)\n",
    "    def standard_resample(arr):\n",
    "        return resample(arr, 2500)\n",
    "    ## if some is not with dim 625, resample it\n",
    "    xl_new = []\n",
    "    for sample in range(xlist.shape[0]):\n",
    "        points = xlist[sample]\n",
    "        if points.shape[1] != 2500:\n",
    "            print(\"resample\")\n",
    "            print(points.shape)\n",
    "            points = np.apply_along_axis(standard_resample, axis=1, arr=points)\n",
    "        points = points.reshape((points.shape[0], 625, 4))\n",
    "        xl_new.append(points)\n",
    "    return np.array(xl_new)\n",
    "\n",
    "## input is after split\n",
    "def apply_resample(xlist, outdim):\n",
    "    ## resample\n",
    "    def resample_waveform(arr):\n",
    "        ## arr.shape: (indim, )\n",
    "        return resample(arr, outdim)\n",
    "    xl_new = []\n",
    "    for sample in range(xlist.shape[0]):\n",
    "        points = xlist[sample]\n",
    "        points = np.apply_along_axis(resample_waveform, axis=1, arr=points)\n",
    "        xl_new.append(points)\n",
    "    return np.array(xl_new)\n",
    "\n",
    "## input is combined exp. (18000 ,625, 4)\n",
    "def get_xtrain_mean(x_train):\n",
    "    ## mean value for each dimension (exp. each of 625 dim)\n",
    "    m = np.mean(x_train, axis=0)\n",
    "    ## then we can apply x_train - m for zero mean\n",
    "    return m\n",
    "\n",
    "## input is after split\n",
    "## one variance for each channel\n",
    "def normalize_waveform():\n",
    "    ## we don't necessarily need this\n",
    "    pass\n",
    "\n",
    "def combine_samples(arrs):\n",
    "    ## exp. arrs.shape: (20, ?)\n",
    "    pass\n",
    "\n",
    "def binary_label(ylist):\n",
    "    ## 1, 2 --> 1\n",
    "    ylist_new = []\n",
    "    for sample in range(ylist.shape[0]):\n",
    "        labels = ylist[sample]\n",
    "        labels[labels > 1] = 1\n",
    "        ylist_new.append(labels)\n",
    "    return np.array(ylist_new)\n",
    "\n",
    "def combine_samples(arrs):\n",
    "    ## exp. arrs.shape: (20, ?)\n",
    "    if arrs.shape[0] < 1:\n",
    "        return arrs\n",
    "    sp = list(arrs[0].shape)\n",
    "    sp[0] = 0\n",
    "    combined = np.zeros(sp)\n",
    "    print(\"combinde\", combined.shape)\n",
    "    for sample in range(arrs.shape[0]):\n",
    "        arr = arrs[sample]\n",
    "        combined = np.concatenate((combined, arr), axis=0)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## global parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### models to test\n",
    "* VERY DEEP CONVOLUTIONAL NEURAL NETWORKS FOR RAW WAVEFORMS\n",
    "\n",
    "they have a clearly defined structure, and their data are of similar dimentions\n",
    "\n",
    "* Raw Waveform-based Audio Classification Using Sample-level CNN Architectures\n",
    "* SAMPLE-LEVEL DEEP CONVOLUTIONAL NEURAL NETWORKS FOR MUSIC AUTO-TAGGING USING RAW WAVEFORMS\n",
    "\n",
    "realtively simple arch;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test raw waveform input first\n",
    "* input: 2500 * 1 waveform (normalized, center to 0, variance 1)\n",
    "* conv layer: with/withour overlapping. In the paper:\n",
    "    * filter size 3, stride 3, 128 filters\n",
    "    * filter size 80, stride 4, 256 filters\n",
    "* batch normalization: after every conv layer\n",
    "* max pool\n",
    "    * stride of 3? 4?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input\n",
    "or we can make the input as 625 * 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use a wrong validation method (mix patients up) just to see whether this network has similar performance with prvious experiments (SVMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model\n",
    "## 1d conv, size 4 filter, 64 filters, stride 2\n",
    "## output 1250 * 64\n",
    "## batch norm\n",
    "## maxpool 2 * 1\n",
    "## output 625 * 64\n",
    "## 1d conv, size 3 filter, stride 2, 128 filters\n",
    "## maxpool 2 * 1\n",
    "## output 312 * 64\n",
    "## 1d conv, size 3 filter, stride 3, 128 filters\n",
    "## output 104 * 128\n",
    "## maxpool 2 * 1\n",
    "## output 52 * 128\n",
    "## 1d conv, size 3 filter, stride 2, 256 filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## one time parameter for the model below\n",
    "## regularizer\n",
    "## l2\n",
    "ker_reg = 0.001\n",
    "act_reg = 0.1\n",
    "## kernel_initializer\n",
    "ker_init = initializers.glorot_normal(seed=None)\n",
    "## shape\n",
    "in_shape = (648, 4)\n",
    "## learning rate\n",
    "opt = Adam()\n",
    "opt.lr = 0.0001\n",
    "##\n",
    "OUTPUT_SIZE = 2\n",
    "##\n",
    "epochs = 25\n",
    "## callback\n",
    "model_callback = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False)\n",
    "csv_logger = CSVLogger(\"./logs/cnn_1_proof.log.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model\n",
    "## resample data to 648 * 1\n",
    "model = Sequential()\n",
    "## 1d conv, size 3 filter, 64 filters, stride 1\n",
    "## batch norm, batch after activation\n",
    "## no maxpool\n",
    "## keras.layers.Conv1D(filters, kernel_size, strides=1, padding='valid', dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "model.add(Convolution1D(filters=64, kernel_size=3, strides=1, padding='same', input_shape=in_shape, kernel_initializer=ker_init, activation='relu', kernel_regularizer=regularizers.l2(ker_reg)))\n",
    "model.add(BatchNormalization())\n",
    "## 1d conv, size 3 filter, 128 filters, stride 1\n",
    "## batch norm, batch after activation\n",
    "## maxpool 3 --> 216 * 128\n",
    "model.add(Convolution1D(filters=128, kernel_size=3, strides=1, padding='same', input_shape=in_shape, activation='relu', kernel_regularizer=regularizers.l2(ker_reg)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "## 1d conv, size 3 filter, 128 filters, stride 2\n",
    "## batch norm, batch after activation\n",
    "## max pool 3 -->  36 * 128\n",
    "model.add(Convolution1D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu', kernel_regularizer=regularizers.l2(ker_reg)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "## 1d conv, size 3 filter, 256 filters, stride 2\n",
    "## batch norm, batch after activation\n",
    "## max pool 3 -->  6 * 256\n",
    "model.add(Convolution1D(filters=256, kernel_size=3, strides=2, padding='same', activation='relu', kernel_regularizer=regularizers.l2(ker_reg)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "## 1d conv, size 3 filter, 512 filters, stride 2\n",
    "## batch norm, batch after activation\n",
    "## max pool 3 -->  1 * 512\n",
    "model.add(Convolution1D(filters=512, kernel_size=3, strides=2, padding='same',activation='relu', kernel_regularizer=regularizers.l2(ker_reg)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "##\n",
    "model.add(Flatten())\n",
    "## fully connected\n",
    "model.add(Dense(OUTPUT_SIZE))\n",
    "## softmax\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 648, 64)           832       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 648, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 648, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 648, 128)          512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 216, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 108, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 108, 128)          512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 36, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 18, 256)           98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 18, 256)           1024      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 6, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 3, 512)            393728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 3, 512)            2048      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1, 512)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1026      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 572,482\n",
      "Trainable params: 570,306\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resample\n",
      "(2490, 1920)\n",
      "resample\n",
      "(878, 1920)\n",
      "(2394, 648, 4)\n",
      "(4144, 648, 4)\n",
      "(3302, 648, 4)\n",
      "(1272, 648, 4)\n",
      "(389, 648, 4)\n",
      "(2716, 648, 4)\n",
      "(61, 648, 4)\n",
      "(628, 648, 4)\n",
      "(611, 648, 4)\n",
      "(771, 648, 4)\n",
      "(201, 648, 4)\n",
      "(1354, 648, 4)\n",
      "(2490, 648, 4)\n",
      "(878, 648, 4)\n",
      "(2506, 648, 4)\n",
      "(1688, 648, 4)\n",
      "(2067, 648, 4)\n",
      "(1554, 648, 4)\n",
      "(635, 648, 4)\n",
      "(1439, 648, 4)\n"
     ]
    }
   ],
   "source": [
    "## experiment\n",
    "## case 12, 14 for test\n",
    "## case 18 for validation\n",
    "## other case for traning\n",
    "x_list, y_list = load_waveforms()\n",
    "x_list = positve_samples(x_list)\n",
    "x_list = split_by_channel(x_list)\n",
    "x_list = apply_resample(x_list, 648)\n",
    "y_list = binary_label(y_list)\n",
    "for i in range(x_list.shape[0]):\n",
    "    print(x_list[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combinde (0, 648, 4)\n",
      "combinde (0,)\n"
     ]
    }
   ],
   "source": [
    "val_idx = []\n",
    "test_idx = []\n",
    "train_list_x = []\n",
    "train_list_y = []\n",
    "val_list_x = []\n",
    "val_list_y = []\n",
    "test_list_x = []\n",
    "test_list_y = []\n",
    "for idx in range(x_list.shape[0]):\n",
    "    if idx not in (val_idx + test_idx):\n",
    "        train_list_x.append(x_list[idx])\n",
    "        train_list_y.append(y_list[idx])\n",
    "        \n",
    "# for idx in val_idx:\n",
    "#     val_list_x.append(x_list[idx])\n",
    "#     val_list_y.append(y_list[idx])\n",
    "    \n",
    "# for idx in test_idx:\n",
    "#     test_list_x.append(x_list[idx])\n",
    "#     test_list_y.append(y_list[idx])  \n",
    "\n",
    "train_list_x = np.array(train_list_x)\n",
    "train_list_y = np.array(train_list_y)\n",
    "# val_list_x = np.array(val_list_x)\n",
    "# val_list_y = np.array(val_list_y)\n",
    "# test_list_x = np.array(test_list_x)\n",
    "# test_list_y = np.array(test_list_y)\n",
    "train_list_x = combine_samples(train_list_x)\n",
    "train_list_y = combine_samples(train_list_y)\n",
    "# val_list_x = combine_samples(val_list_x)\n",
    "# val_list_y = combine_samples(val_list_y)\n",
    "# test_list_x = combine_samples(test_list_x)\n",
    "# test_list_y = combine_samples(test_list_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_idx = np.arange(train_list_x.shape[0])\n",
    "np.random.shuffle(shuffle_idx)\n",
    "train_list_x = train_list_x[shuffle_idx]\n",
    "train_list_y = train_list_y[shuffle_idx]\n",
    "## train val test split\n",
    "val_start = int(train_list_x.shape[0] * 0.7)\n",
    "val_end = int(train_list_x.shape[0] * 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_list_x = train_list_x[val_start: val_end,:]\n",
    "val_list_y = train_list_y[val_start: val_end]\n",
    "test_list_x = train_list_x[val_end:, :]\n",
    "test_list_y = train_list_y[val_end:]\n",
    "train_list_x = train_list_x[: val_start, :]\n",
    "train_list_y = train_list_y[: val_start]\n",
    "\n",
    "train_list_y = np_utils.to_categorical(train_list_y, num_classes=2)\n",
    "val_list_y = np_utils.to_categorical(val_list_y, num_classes=2)\n",
    "test_list_y = np_utils.to_categorical(test_list_y, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21770 samples, validate on 6220 samples\n",
      "Epoch 1/25\n",
      " - 19s - loss: 0.9122 - acc: 0.9278 - val_loss: 0.8307 - val_acc: 0.9450\n",
      "Epoch 2/25\n",
      " - 13s - loss: 0.7889 - acc: 0.9513 - val_loss: 0.7935 - val_acc: 0.9375\n",
      "Epoch 3/25\n",
      " - 13s - loss: 0.7264 - acc: 0.9567 - val_loss: 0.7391 - val_acc: 0.9439\n",
      "Epoch 4/25\n",
      " - 13s - loss: 0.6744 - acc: 0.9591 - val_loss: 0.6878 - val_acc: 0.9444\n",
      "Epoch 5/25\n",
      " - 13s - loss: 0.6314 - acc: 0.9599 - val_loss: 0.6478 - val_acc: 0.9431\n",
      "Epoch 6/25\n",
      " - 13s - loss: 0.5873 - acc: 0.9639 - val_loss: 0.6008 - val_acc: 0.9487\n",
      "Epoch 7/25\n",
      " - 13s - loss: 0.5491 - acc: 0.9648 - val_loss: 0.6333 - val_acc: 0.9315\n",
      "Epoch 8/25\n",
      " - 13s - loss: 0.5106 - acc: 0.9678 - val_loss: 0.5434 - val_acc: 0.9519\n",
      "Epoch 9/25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-fcc16c72a570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_list_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_list_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           callbacks=[model_callback, csv_logger])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Works/breast/simpletensor/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Works/breast/simpletensor/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Works/breast/simpletensor/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Works/breast/simpletensor/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Works/breast/simpletensor/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_list_x, train_list_y,\n",
    "          epochs=epochs,\n",
    "          verbose=2,\n",
    "          validation_data=(val_list_x, val_list_y),\n",
    "          callbacks=[model_callback, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(test_list_x, test_list_y)\n",
    "print(\"loss\", loss, \"acc\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simpletensor",
   "language": "python",
   "name": "simpletensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
